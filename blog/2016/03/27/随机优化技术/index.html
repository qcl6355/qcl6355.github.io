<!DOCTYPE html>
<html lang="en-us">
  <head>
    <title>随机优化技术 - Sheng Li&#39;s Blog</title>
    <meta charset="utf-8" />
    <meta name="author" content="ShengLi" />
    <meta name="description" content="随机优化技术相关记录" />
    <meta name="keywords" content="SGD,AdaGrad,Rmsprop" />
    <link rel="stylesheet" href="/media/css/main.css" type="text/css">
    <link rel="stylesheet" href="/media/css/prettify.css" type="text/css">
  </head>
  <body class="container">
    <div>
      <header class="masthead">
        <h1 class="masthead-title"><a href="/">Sheng Li&#39;s Blog</a></h1>
        <p>Ultimated Blade Works</p>
        <ul>
          <li><a href="/blog/">Blog</a></li>
          <li><a href="/wiki/">Wiki</a></li>
          <li><a href="/tags/">Tags</a></li>
          <li><a href="/about/">About</a></li>
          <li><a href="https://github.com/qcl6355">GitHub</a></li>
          <li><a href="/rss.xml">RSS</a></li>
        </ul>
        <form method="get" id="searchform" action="//www.google.com/search">
          <input type="text" class="field" name="q" id="s" placeholder="Search">
          <input type="hidden" name="as_sitesearch" value="qcl6355.github.io">
        </form>
      </header>
    </div>

<div>
<div class="post">
<h1>随机优化技术</h1>

<div id="outline-container-orgheadline1" class="outline-2">
<h2 id="orgheadline1">几种随机优化技术介绍</h2>
<div class="outline-text-2" id="text-orgheadline1">
<ol class="org-ol">
<li><p>
Stochastic Gradient Descent (SGD)
</p>

<p>
SGD simply updates each parameter by substracting the gradient of
the loss w.r.t the parameter, scaled by the learning rate \(\eta\), a
hyperparameter. If \(\eta\) is too large, SGD will diverge; if it's
too small, it will converge slowly. The update rule is simply \[
   \theta_{t+1} = \theta_{t} - \eta \nabla L(\theta{t}) \]
</p></li>

<li><p>
Momentum (动量)
</p>

<p>
In SGD, the gradient \(\nabla L(\theta_{t})\) often changes rapidlly
at each iteration \(t\) due to the fact that the loss is being
computed over different data. This is often partially mitigated by
re-using the gradient value from the previous iteration, scaled by
a momentum hyperparameter \(\mu\), as follows:
\[ v_{t+1} = \mu v_{t} - \eta \nabla L(\theta{t})\]
\[ \theta_{t+1} = \theta_{t} + v_{t+1} \]
It has been argued that including the previous gradient step has
the effect of approximating some second-order information about the
gradient.
</p></li>

<li>Nesterov's Accelerated Gradient (NAG)</li>

<li><p>
AdaGrad
</p>

<p>
Adagrad effectively rescales the learning rate for each parameter
according to the history of the gradients for that parameter. This
is done by dividing each item in \(\nabla L\) by the squares root of
the sum of squares of its historical gradient. Rescaling in this
way effectively lowers the learning rate for parameters which
consistently have large gradient values. It also effectively
decreases the learning rate over time, because the sum of squares
will continue to grow with the iteration. After setting the
rescaling term g = 0, the updates are as follows:
\[ g_{t+1} = g_{t} + \nabla L(\theta_{t})^{2} \]
\[ \theta_{t+1} = \theta_{t} - \frac{\eta \nabla
   L(\theta_{t})}{\sqrt{g_{t+1}} + \epsilon} \]
where division is elementwise and \(\epsilon\) is a small constant
included for numerical stability.
</p></li>
</ol>
</div>
</div>

</div>
</div>
    <div>
      <div class="post-meta">
        <span title="post date" class="post-info">2016-03-27</span>
        <span title="last modification date" class="post-info">2016-03-27</span>
        <span title="tags" class="post-info"><a href="/tags/sgd/">SGD</a>, <a href="/tags/adagrad/">AdaGrad</a>, <a href="/tags/rmsprop/">Rmsprop</a></span>
        <span title="author" class="post-info">ShengLi</span>
      </div>
      <section>
        <h1>Comments</h1>
        <div id="disqus_thread"></div>
        <script type="text/javascript">
          //var disqus_developer = 1;
          var disqus_identifier = "/blog/2016/03/27/随机优化技术";
          var disqus_url = "http://qcl6355.github.io/blog/2016/03/27/随机优化技术";
          var disqus_shortname = 'Sheng';
          /* * * DON'T EDIT BELOW THIS LINE * * */
          (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
          })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
        <a href="//disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
      </section>
      <script src="//code.jquery.com/jquery-latest.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/prettify/r298/prettify.js"></script>
      <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      <script src="/media/js/main.js"></script>
      <div class="footer">
        <p>Generated by <a href="http://www.gnu.org/software/emacs/">Emacs</a> 24.x (<a href="http://orgmode.org">Org mode</a> 8.x)</p>
        <p>
          Copyright &copy; 2012 - <span id="footerYear"></span> <a href="mailto:qcl6355 &lt;at&gt; gmail &lt;dot&gt; com">ShengLi</a>
          &nbsp;&nbsp;-&nbsp;&nbsp;
          Powered by <a href="https://github.com/kelvinh/org-page" target="_blank">org-page</a>
          <script type="text/javascript">document.getElementById("footerYear").innerHTML = (new Date()).getFullYear();</script>
        </p>
      </div>
    </div>

  </body>
</html>
