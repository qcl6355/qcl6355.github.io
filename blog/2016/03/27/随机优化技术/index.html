<!DOCTYPE html>
<html lang="en-us">
<head>
  <title>随机优化技术 - org-page</title>
  <meta charset="utf-8" />
  <meta name="author" content="ShengLi" />
  <meta name="description" content="随机优化技术相关记录" />
  <meta name="keywords" content="SGD,AdaGrad,RMSProp" />

  <link rel="alternate" title="RSS Feed" href="/rss.xml" type="application/rss+xml">
  <link rel="stylesheet" href="/media/css/main.css" type="text/css">
  <link rel="stylesheet" href="/media/css/posts.css" type="text/css">
</head>

  <body class="container">
<header id="header">
<body>
    <nav class="navbar navbar-default navbar-fixed-top" style="opacity: .9" role="navigation">
        <div class="container-fluid">
<div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">org-page</a>
        </div>
<div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                <li class="active"><a href="/">Blog</a></li>
                <li class="active"><a href="/">Wiki</a></li>
          <li><a href="https://github.com/qcl6355">GitHub</a></li>
        </ul>
</div>
</div>
    </nav>
</body>
</header>

<section id="content" role="main">
    <div id="outline-container-sec-" class="row" style="padding-top: 70px">
        <div class="col-md-2"></div>
            <h1>随机优化技术</h1>
            
<div id="outline-container-orgheadline1" class="outline-2">
<h2 id="orgheadline1">几种随机优化技术介绍</h2>
<div class="outline-text-2" id="text-orgheadline1">
<ol class="org-ol">
<li>Stochastic Gradient Descent (SGD):
SGD simply updates each parameter by substracting the gradient of
the loss w.r.t the parameter, scaled by the learning rate \(\eta\), a
hyperparameter. If \(\eta\) is too large, SGD will diverge; if it's
too small, it will converge slowly. The update rule is simply \[
   \theta_{t+1} = \theta_{t} - \eta \nabla L(\theta{t}) \]</li>

<li>Momentum (动量):
In SGD, the gradient \(\nabla L(\theta_{t})\) often changes rapidlly
at each iteration \(t\) due to the fact that the loss is being
computed over different data. This is often partially mitigated by
re-using the gradient value from the previous iteration, scaled by
a momentum hyperparameter \(\mu\), as follows:
\[ v_{t+1} = \mu v_{t} - \eta \nabla L(\theta{t})\]
\[ \theta_{t+1} = \theta_{t} + v_{t+1} \]
It has been argued that including the previous gradient step has
the effect of approximating some second-order information about the
gradient.</li>

<li>AdaGrad:
Adagrad effectively rescales the learning rate for each parameter
according to the history of the gradients for that parameter. This
is done by dividing each item in \(\nabla L\) by the squares root of
the sum of squares of its historical gradient. Rescaling in this
way effectively lowers the learning rate for parameters which
consistently have large gradient values. It also effectively
decreases the learning rate over time, because the sum of squares
will continue to grow with the iteration. After setting the
rescaling term g = 0, the updates are as follows:
\[ g_{t+1} = g_{t} + \nabla L(\theta_{t})^{2} \]
\[ \theta_{t+1} = \theta_{t} - \frac{\eta \nabla
   L(\theta_{t})}{\sqrt{g_{t+1}} + \epsilon} \]
where division is elementwise and \(\epsilon\) is a small constant
included for numerical stability.</li>

<li>RMSProp:
RMSProp is very similar to AdaGrad. The only difference is that the
\(g_{t}\) term is computed as an exponentially decaying average
instead of an accumulated sum. This makes \(g_{t}\) an estimate of
the second moment of \(\nabla L\) and avoids the fact that the
learning rate effectively shrinks over time. The update is as follows:
\[ g_{t+1} = \beta g_{t} + (1 - \beta)\nabla L(\theta_{t})^{2} \]
\[ \theta_{t+1} = \theta_{t} - \frac{\eta \nabla L(\theta_{t})}{\sqrt{g_{t+1}} + \epsilon} \]</li>
</ol>


<p>
In the original lecture slides where it was proposed, \(\beta\) is
set to 0.9. In , it is shown that the \(\sqrt{g_{t+1}}\) term
approximates (in expectation) the diagonal of the absolute value of
the Hessian matrix (assuming the update steps are \(N(0,1)\)
distributed). It is also argued that the absolute value of the
Hessian is better to use for non-convex problems which may have
many saddle points.
</p>
</div>
</div>

    </div>
</section>


<footer id="footer">
    <br />
</footer>

  </body>
</html>
