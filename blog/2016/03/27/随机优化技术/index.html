<!DOCTYPE html>
<html lang="en-us">
<head>
    <title>随机优化技术 - Sheng Li&#39;s Blog</title>
    <meta charset="utf-8" />
    <meta name="author" content="ShengLi" />
        <meta name="description" content="随机优化技术相关记录" />
        <meta name="keywords" content="SGD,AdaGrad,RMSProp" />
    <link rel="stylesheet" href="/media/css/style.css">
    <link rel="stylesheet" href="/media/css/highlight.css">
    <style type="text/css"></style>
</head>

  <body class="container">
    <nav class="main-nav">
        <a href="/blog">Archive</a>
        <a href="/tags">Tag</a>
        <a href="/about/">About</a>
        <a class="cta" href="https://github.com/qcl6355">GitHub</a>
    </nav>

<div>
    <section id="wrapper">
        <article class="post">
            <header>
                    <h1>随机优化技术</h1>
            </header>
            <br />
            <section id="post-body">
                
<div id="outline-container-orgheadline1" class="outline-2">
<h2 id="orgheadline1">几种随机优化技术介绍</h2>
<div class="outline-text-2" id="text-orgheadline1">
<ol class="org-ol">
<li>Stochastic Gradient Descent (SGD):
SGD simply updates each parameter by substracting the gradient of
the loss w.r.t the parameter, scaled by the learning rate \(\eta\), a
hyperparameter. If \(\eta\) is too large, SGD will diverge; if it's
too small, it will converge slowly. The update rule is simply \[
   \theta_{t+1} = \theta_{t} - \eta \nabla L(\theta{t}) \]</li>

<li>Momentum (动量):
In SGD, the gradient \(\nabla L(\theta_{t})\) often changes rapidlly
at each iteration \(t\) due to the fact that the loss is being
computed over different data. This is often partially mitigated by
re-using the gradient value from the previous iteration, scaled by
a momentum hyperparameter \(\mu\), as follows:
\[ v_{t+1} = \mu v_{t} - \eta \nabla L(\theta{t})\]
\[ \theta_{t+1} = \theta_{t} + v_{t+1} \]
It has been argued that including the previous gradient step has
the effect of approximating some second-order information about the
gradient.</li>

<li>AdaGrad:
Adagrad effectively rescales the learning rate for each parameter
according to the history of the gradients for that parameter. This
is done by dividing each item in \(\nabla L\) by the squares root of
the sum of squares of its historical gradient. Rescaling in this
way effectively lowers the learning rate for parameters which
consistently have large gradient values. It also effectively
decreases the learning rate over time, because the sum of squares
will continue to grow with the iteration. After setting the
rescaling term g = 0, the updates are as follows:
\[ g_{t+1} = g_{t} + \nabla L(\theta_{t})^{2} \]
\[ \theta_{t+1} = \theta_{t} - \frac{\eta \nabla
   L(\theta_{t})}{\sqrt{g_{t+1}} + \epsilon} \]
where division is elementwise and \(\epsilon\) is a small constant
included for numerical stability.</li>

<li>RMSProp:
RMSProp is very similar to AdaGrad. The only difference is that the
\(g_{t}\) term is computed as an exponentially decaying average
instead of an accumulated sum. This makes \(g_{t}\) an estimate of
the second moment of \(\nabla L\) and avoids the fact that the
learning rate effectively shrinks over time. The update is as follows:
\[ g_{t+1} = \beta g_{t} + (1 - \beta)\nabla L(\theta_{t})^{2} \]
\[ \theta_{t+1} = \theta_{t} - \frac{\eta \nabla L(\theta_{t})}{\sqrt{g_{t+1}} + \epsilon} \]</li>
</ol>


<p>
In the original lecture slides where it was proposed, \(\beta\) is
set to 0.9. In , it is shown that the \(\sqrt{g_{t+1}}\) term
approximates (in expectation) the diagonal of the absolute value of
the Hessian matrix (assuming the update steps are \(N(0,1)\)
distributed). It is also argued that the absolute value of the
Hessian is better to use for non-convex problems which may have
many saddle points.
</p>
</div>
</div>

            </section>
    </section>
</div>

    <div>
    <section id="wrapper">
      <div class="post-meta">
        <span title="post date" class="post-info">2016-03-27</span>
        <span title="tags" class="post-info"><a href="/tags/sgd/">SGD</a>, <a href="/tags/adagrad/">AdaGrad</a>, <a href="/tags/rmsprop/">RMSProp</a></span>
      </div>
      <br />
      <br />
      <section>
        <h1>Comments</h1>
        <div id="disqus_thread"></div>
        <script type="text/javascript">
          //var disqus_developer = 1;
          var disqus_identifier = "/blog/2016/03/27/随机优化技术";
          var disqus_url = "http://qcl6355.github.io/blog/2016/03/27/随机优化技术";
          var disqus_shortname = 'Sheng';
          /* * * DON'T EDIT BELOW THIS LINE * * */
          (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
          })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
      </section>
      <script src="http://code.jquery.com/jquery-latest.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/prettify/r298/prettify.js"></script>
      <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      <script src="/media/js/main.js"></script>
      <div align="center">
      <footer id="footer">
        <p class="small">
          Copyright &copy; 2012 - <span id="footerYear"></span> <a href="mailto:qcl6355 &lt;at&gt; gmail &lt;dot&gt; com">ShengLi</a>
          &nbsp;&nbsp;-&nbsp;&nbsp;
          Powered by <a href="https://github.com/kelvinh/org-page" target="_blank">org-page</a>
          <script type="text/javascript">document.getElementById("footerYear").innerHTML = (new Date()).getFullYear();</script>
        </p>
        <br />
      </footer>
      </div>
      </section>
    </div>

  </body>
</html>
