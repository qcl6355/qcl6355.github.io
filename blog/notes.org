#+TITLE:       个人笔记
#+AUTHOR:      ShengLi
#+EMAIL:       qcl6355@gmail.com
#+DATE:        2016-03-27 Sun
#+URI:         /blog/%y/%m/%d/个人笔记
#+KEYWORDS:    <TODO: insert your keywords here>
#+TAGS:        Emacs,MXNet,C++,Java,Javascript,Rabit
#+LANGUAGE:    en
#+OPTIONS:     H:3 num:nil toc:nil \n:nil ::t |:t ^:nil -:nil f:t *:t <:t
#+DESCRIPTION: 一些问题的解决方法

* MXNet编译相关
** no 'return_type' compile error
可能是子模块的版本不对应造成的，解决方法：git pull & git submodule update & make clean & make -j

* D3可视化学习
1. 动态效果
| 方法         | 说明                                              |
|--------------+---------------------------------------------------|
| transition() | 状态转移                                          |
| duration()   | 指定过渡的持续时间，单位为毫秒                    |
| ease()       | 指定过渡的方式（linear, circle, elastic, bounce） |
| delay()      | 指定延迟的时间，表示一定时间后才开始转变          | 

#+BEGIN_SRC javascript
  var circle1 = svg.append("circle")
      .attr("cx", 100)
      .attr("cy", 100)
      .attr("r", 45)
      .style("fill", "green")

  circle1.transition()
      .duration(1000)
      .attr("cx", 300)
      .style("fill", "steelblue")
#+END_SRC

2. 理解Update, Enter, Exit的用法
处理选择的元素和数据的数量关系不确定的情况
update表示对齐的部分
1. 元素的个数 < 数据的个数， 使用enter，添加元素
2. 元素的个数 > 数据的个数， 使用exit，删除元素

#+BEGIN_SRC javascript
  // 假定body里面有三个p元素

  var dataset = [3, 6, 9, 12, 15];

  // 选择body中的p元素
  var p = d3.select("body").selectAll("p");

  // 获取update部分
  var update = p.data(dataset);

  // 获取enter部分
  var enter = update.enter();
  // 如果元素p的个数大于dataset的个数时
  var exit = update.exit();

  // update部分的处理：更新属性值
  update.text(function(d) {
      return "update " + d;
  });

  // enter部分的处理；添加元素后赋予属性值
  enter.append("p")
      .text(function(d) {
          return "enter " + d;
      });
#+END_SRC

3. 交互式操作
在d3中，每一个选择集都有on()函数，用于添加事件监听器。
#+BEGIN_SRC javascript
  var circle = svg.append("circle")

  circle.on("click", function(){
      //add some processing code
  });
#+END_SRC

4. 布局（layout）
Layout: 决定什么元素绘制在哪里
数据->布局->获得绘图所需的数据->在画布上添加相应的图形->图表
| Layout    | 说明     |
|-----------+----------|
| Pie       | 饼状图   |
| Force     | 力导向图 |
| Chord     | 弦图     |
| Tree      | 树状图   |
| Cluster   | 集群图   |
| Bundle    | 捆图     |
| Pack      | 打包图   |
| Histogram | 直方图   |
| Partition | 分区图   |
| Stack     | 堆栈图   |
| Treemap   | 矩阵树图 |
| Hierarchy | 层级图   |

* 随机优化技术
1. Stochastic Gradient Descent (SGD)
   SGD simply updates each parameter by substracting the gradient of
   the loss w.r.t the parameter, scaled by the learning rate $\eta$, a
   hyperparameter. If $\eta$ is too large, SGD will diverge; if it's
   too small, it will converge slowly. The update rule is simply \[
   \theta_{t+1} = \theta_{t} - \eta \nabla L(\theta{t}) \]

2. Momentum (动量)
   In SGD, the gradient $\nabla L(\theta_{t})$ often changes rapidlly
   at each iteration $t$ due to the fact that the loss is being
   computed over different data. This is often partially mitigated by
   re-using the gradient value from the previous iteration, scaled by
   a momentum hyperparameter $\mu$, as follows:
   \[ v_{t+1} = \mu v_{t} - \eta \nabla L(\theta{t})\]
   \[ \Theta_{t+1} = \theta_{t} + v_{t+1} \]
   It has been argued that including the previous gradient step has
   the effect of approximating some second-order information about the
   gradient.

3. Nesterov's Accelerated Gradient (NAG)

4. AdaGrad
   Adagrad effectively rescales the learning rate for each parameter
   according to the history of the gradients for that parameter. This
   is done by dividing each item in $\nabla L$ by the squares root of
   the sum of squares of its historical gradient. Rescaling in this
   way effectively lowers the learning rate for parameters which
   consistently have large gradient values. It also effectively
   decreases the learning rate over time, because the sum of squares
   will continue to grow with the iteration. After setting the
   rescaling term g = 0, the updates are as follows:
   \[ g_{t+1} = g_{t} + \nabla L(\theta_{t})^{2} \]
   \[ \theta_{t+1} = \theta_{t} - \frac{\eta \nabla
   L(\theta_{t})}{\sqrt{g_{t+1}} + \epsilon} \]
   where division is elementwise and $\epsilon$ is a small constant
   included for numerical stability.
* rabit框架使用总结
1. Allreduce peforms reduction across different computation nodes and
returns the result to every node.
2. Broadcast is another method, this function allows one node to broadcast
its local data to all other nodes.
#+BEGIN_SRC c++
  #include <rabit.h>

  int main(int argc, char *argv[]) {
    rabit::Init(argc, argv);
    // load the latest checked model
    int version = rabit::LoadCheckPoint(&model);

    // initialize the model if it is the first version
    if (version == 0) model.InitModel();
    // the version number marks the iteration to resume
    for (int iter = version; iter < max_iter; ++iter) {
      // at this point, the model object should allow us to recover the program state

      // each iteration can contain multiple calls of allreduce/broadcast
      rabit::Allreduce<rabit::op::Max>(&data[0], n);

      // checkpoint model after one iteration finishes
      rabit::Checkpoint(&model);
    }
    rabit::Finalize();
    return 0;
  }

#+END_SRC
